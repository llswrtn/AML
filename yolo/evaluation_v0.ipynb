{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1a3e473-3dea-473a-bde5-e153a19d12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "#import progressbar\n",
    "import pydicom as dicom\n",
    "#import pylibjpeg\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from typing import List, Tuple, Dict, Optional, Any\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2327127a-53ec-46cb-b65d-75beb5025a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/media/luisa/Volume/AML/models/fasterrcnn_resnet50_fpn_10_epochs_diffNoBox_v01/model/fasterrcnn_resnet50_fpn_10_epochs_diffNoBox_v0.pth\"\n",
    "TRAINSET_INDICES_PATH = \"/media/luisa/Volume/AML/models/fasterrcnn_resnet50_fpn_10_epochs_diffNoBox_v01/test_set_fasterrcnn_resnet50_fpn_10_epochs_diffNoBox_v0.csv\"\n",
    "\n",
    "ROOT =  '/media/luisa/Volume/AML/siim-covid19-detection'\n",
    "CLEAN_TRAIN_PATH = '/media/luisa/Volume/AML/train_image_level_clean_paths.csv'\n",
    "#CLEAN_TRAIN_PATH = '/media/luisa/Volume/AML/train_image_level_clean_paths_NOTNA.csv'\n",
    "\n",
    "CLASSES = ['Negative for Pneumonia',' Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']\n",
    "\n",
    "#parameters for evaluation\n",
    "MIN_IOU = 0.5\n",
    "NMS_THRESHOLD = 0.3\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model  = torchvision.models.detection.fasterrcnn_resnet50_fpn(max_size =240 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f78d4d49-6138-4c5f-be68-70be7ae3c991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_id'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id\n",
       "0      4287\n",
       "1      3093\n",
       "2       648\n",
       "3       988\n",
       "4      3050"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### list of all image_ids that are used in test set (ids can be anything that unique identifies an image of the test set to map gt and predictions) ####\n",
    "indices_df = pd.read_csv(TRAINSET_INDICES_PATH)\n",
    "\n",
    "# remove these lines if df colum naming is fixed during training\n",
    "indices_df = indices_df.drop(['Unnamed: 0'], axis = 1)\n",
    "indices_df = indices_df.rename(columns={\"0\": 'image_id'}, errors=\"raise\")\n",
    "print(indices_df.columns)\n",
    "indices_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e400a45-cd36-49f5-941c-bc91d3b9ed25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load from model state dict\n",
    "\n",
    "'''\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "'''\n",
    "# load entire model \n",
    "\n",
    "model = torch.load(MODEL_PATH)\n",
    "#model.to(device)\n",
    "model.eval()\n",
    "print()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5bec3d-7af1-4a34-b1ba-c818d67ba9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for torchvision faster rcnn adapted from official repo\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(ToTensor())\n",
    "    #no data augmentation for now, maybe implement later\n",
    "    #if train:\n",
    "        #transforms.append(RandomHorizontalFlip(0.5))\n",
    "    return Compose(transforms)\n",
    "\n",
    "class ToTensor(torch.nn.Module):\n",
    "    def forward(self, image: torch.Tensor,\n",
    "                target: Optional[Dict[str, torch.Tensor]] = None) -> Tuple[torch.Tensor, Optional[Dict[str, torch.Tensor]]]:\n",
    "        image = to_tensor(image)\n",
    "        return image, target\n",
    "    \n",
    "def to_tensor(pic):\n",
    "    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n",
    "    This function does not support torchscript.\n",
    "\n",
    "    See :class:`~torchvision.transforms.ToTensor` for more details.\n",
    "\n",
    "    Args:\n",
    "        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Converted image.\n",
    "    \"\"\"\n",
    "    #if not(_is_pil_image(pic) or _is_numpy(pic)):\n",
    "        #raise TypeError('pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\n",
    "\n",
    "    #if not(_is_numpy(pic)):\n",
    "        #raise TypeError('pic should be PIL Image or ndarray. Got {}'.format(type(pic)))\n",
    "\n",
    "    #if _is_numpy(pic) and not _is_numpy_image(pic):\n",
    "        #raise ValueError('pic should be 2/3 dimensional. Got {} dimensions.'.format(pic.ndim))\n",
    "\n",
    "    default_float_dtype = torch.get_default_dtype()\n",
    "\n",
    "    if isinstance(pic, np.ndarray):\n",
    "        # handle numpy array\n",
    "        if pic.ndim == 2:\n",
    "            pic = pic[:, :, None]\n",
    "\n",
    "        img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
    "        # backward compatibility\n",
    "        if isinstance(img, torch.ByteTensor):\n",
    "            return img.to(dtype=default_float_dtype).div(255)\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    if accimage is not None and isinstance(pic, accimage.Image):\n",
    "        nppic = np.zeros([pic.channels, pic.height, pic.width], dtype=np.float32)\n",
    "        pic.copyto(nppic)\n",
    "        return torch.from_numpy(nppic).to(dtype=default_float_dtype)\n",
    "\n",
    "    # handle PIL Image\n",
    "    mode_to_nptype = {'I': np.int32, 'I;16': np.int16, 'F': np.float32}\n",
    "    img = torch.from_numpy(\n",
    "        np.array(pic, mode_to_nptype.get(pic.mode, np.uint8), copy=True)\n",
    "    )\n",
    "\n",
    "    if pic.mode == '1':\n",
    "        img = 255 * img\n",
    "    img = img.view(pic.size[1], pic.size[0], len(pic.getbands()))\n",
    "    # put it from HWC to CHW format\n",
    "    img = img.permute((2, 0, 1)).contiguous()\n",
    "    if isinstance(img, torch.ByteTensor):\n",
    "        return img.to(dtype=default_float_dtype).div(255)\n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "805de599-1f56-44af-b8bd-a47c0b89397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset that contains only image_ids of test set\n",
    "\n",
    "class Test_only_ImageLevelSiimCovid19Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root = ROOT, transforms = get_transform(train=True)):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        #use only indices that are part of previous test set\n",
    "        test_indices = indices_df['image_id']\n",
    "        test_set  = pd.read_csv(CLEAN_TRAIN_PATH)\n",
    "        \n",
    "        test_set = test_set[test_set.index.isin(test_indices)]\n",
    "\n",
    "        # .csv under CLEAN_TRAIN_PATH contains all image paths with their bounding box annotations\n",
    "        #self.imgs = pd.read_csv(CLEAN_TRAIN_PATH)['path'].tolist()\n",
    "        self.imgs = test_set['path'].tolist()\n",
    "        #self.annotations = pd.read_csv(CLEAN_TRAIN_PATH)['boxes'].tolist()\n",
    "        self.annotations = test_set['boxes'].tolist()\n",
    "        \n",
    "        self.study_class_labels = test_set['study_label'].tolist()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # load images and annotations\n",
    "    \n",
    "        img_path = self.imgs[idx]\n",
    "        img = np.float32(dicom.dcmread(img_path).pixel_array)\n",
    "        \n",
    "        study_class_label = self.study_class_labels[idx]\n",
    "        \n",
    "        if self.annotations[idx] is np.NaN:\n",
    "            boxes_dict = []\n",
    "            \n",
    "        else:\n",
    "            boxes_dict = ast.literal_eval(self.annotations[idx])\n",
    "            \n",
    "        num_obj = len(boxes_dict)\n",
    "    \n",
    "        boxes = []\n",
    "        for i in boxes_dict:\n",
    "\n",
    "            x_min = i['x']\n",
    "            y_min = i['y']\n",
    "            x_max = i['x'] + i['width']\n",
    "            y_max = i['y'] + i['height']\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "                \n",
    "        #only one class label at this step: opacity\n",
    "        labels = torch.ones((num_obj,), dtype=torch.int64)\n",
    "        iscrowd = torch.zeros((num_obj,), dtype=torch.int64) # assuming there are no crowds\n",
    "        \n",
    "      \n",
    "        #handle images without boxes in annotation\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "            \n",
    "        \n",
    "        '''\n",
    "        if len(boxes) == 0:\n",
    "            boxes = [[0, 0, 1, 1]]\n",
    "            labels = torch.zeros(1, dtype=torch.int64)  # label for background is 0\n",
    "            #iscrowd = torch.zeros((1,), dtype=torch.int64) # assuming there are no crowds\n",
    "            #boxes = []\n",
    "            #labels = torch.zeros(0, dtype=torch.int64)  # label for background is 0\n",
    "        '''    \n",
    "      \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        study_label = 0\n",
    "        if study_class_label == 'Typical Appearance':\n",
    "            study_label = 1\n",
    "        if study_class_label == 'Indeterminate Appearance':\n",
    "            study_label = 2\n",
    "        if study_class_label == 'Atypical Appearance':\n",
    "            study_label = 3\n",
    "\n",
    "        study_label = torch.tensor([study_label])\n",
    "        \n",
    "        \n",
    "        image_id = torch.tensor([idx])\n",
    "        \n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        \n",
    "        target[\"area\"] = area\n",
    "        \n",
    "        target[\"study_label\"] = study_label\n",
    "        '''\n",
    "        target['iscrowd'] = iscrowd\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # TODO: transforms for test set\n",
    "        #transform_img = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "        #img = transform_img(img)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)        \n",
    "        \n",
    "\n",
    "        return img, target\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43477b77-19c4-4a1e-979d-71cbb9b1a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom colate_fn \n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b4ebb58-90c9-4be4-ab57-7420a725ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader for test set\n",
    "dataset_test = Test_only_ImageLevelSiimCovid19Dataset(ROOT, get_transform(train=False))\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=True, num_workers=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "738f5cb6-9f37-41ee-9978-3d34881a1085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.9 s, sys: 8.79 s, total: 52.7 s\n",
      "Wall time: 3min 42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "<timed exec>:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#### create inputs for evaluation ####\n",
    "\n",
    "# 1.) all_preds_df: DataFrame with all predicted boxes ('pred_boxes'), their confidence scores ('pred_scores') and their 'image_id' (unique identifier of image in which box was predicted)\n",
    "#     all_pred_boxes_df HAS TO BE sorted in ascending order of pred_scores!\n",
    "\n",
    "# 2.) all_gt_boxes: array of [image_id, gt_box, used_flag], for each gt box. all used_flags are initialy set to False\n",
    "\n",
    "\n",
    "bar =progressbar.ProgressBar()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_preds = [] #all predicted boxes with image_id and score\n",
    "#gt_dict = dict() #key: image_id, value: array with gt boxes\n",
    "\n",
    "all_gt_boxes = [] # all gt boxes  [image_id, box, used]\n",
    "\n",
    "for batch in bar(data_loader_test):\n",
    "    \n",
    "    images, targets = batch\n",
    "\n",
    "    images = list(img.to(device) for img in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    \n",
    "    \n",
    "\n",
    "    boxes = targets[0]['boxes'].cpu().numpy().astype(np.int32)\n",
    "\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "    outputs = model(images)\n",
    "    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "    #print(\"1\")\n",
    "   \n",
    "    \n",
    "    box_candidates = outputs[0]['boxes']\n",
    "    scores_candidates = outputs[0]['scores']\n",
    "    \n",
    "   \n",
    "    # apply non-maximum suppression  \n",
    "    keep = torchvision.ops.nms(box_candidates, scores_candidates, NMS_THRESHOLD)\n",
    "    \n",
    "    \n",
    "    pred_boxes = []\n",
    "    pred_scores = []\n",
    "    for b in keep :\n",
    "        pred_b = box_candidates[b].cpu().detach().numpy()\n",
    "        #print(\"2\")\n",
    "        pred_s = scores_candidates[b].cpu().detach().numpy()\n",
    "        pred_boxes.append(pred_b)\n",
    "        pred_scores.append(pred_s)\n",
    "  \n",
    "    #pred_boxes = outputs[0]['boxes'].detach().numpy()\n",
    "    #pred_scores = outputs[0]['scores'].detach().numpy()\n",
    "    \n",
    "    image_id = targets[0]['image_id'].cpu().numpy()\n",
    "    image_id = image_id[0]\n",
    "    gt_boxes = targets[0]['boxes'].cpu().numpy()\n",
    "    \n",
    "    #gt_dict[image_id] = gt_boxes\n",
    "    gt_box = []\n",
    "    for b in range(len(gt_boxes)):\n",
    "        gt_boxes_id = [image_id, gt_boxes[b], False]\n",
    "        #gt_box.append(gt_boxes_id)\n",
    "        all_gt_boxes.append(gt_boxes_id)   \n",
    "    #all_gt_boxes.append(gt_box)    \n",
    "        \n",
    "    for b in range(len(pred_boxes)):\n",
    "        boxes_scores_id = [image_id, pred_boxes[b], pred_scores[b]]\n",
    "        all_preds.append(boxes_scores_id)\n",
    "        \n",
    "\n",
    "all_preds_df = pd.DataFrame(all_preds, columns=['image_id', 'pred_boxes', 'pred_scores'])    \n",
    "all_preds_df = all_preds_df.sort_values('pred_scores',  ascending=False)\n",
    "\n",
    "#all_gt_boxes_df = pd.DataFrame(all_gt_boxes, columns=['image_id', 'gt_boxes', 'used']) \n",
    "\n",
    "# inices = all_gt_boxes[0]\n",
    "# inices = all_gt_boxes[1]\n",
    "# inices = all_gt_boxes[2]\n",
    "all_gt_boxes_ = np.array(all_gt_boxes).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d4b844d-0a8d-4279-a301-ac733d6ec456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14537\n",
      "5862\n"
     ]
    }
   ],
   "source": [
    "print(len(all_preds_df))\n",
    "print(len(all_preds_df[all_preds_df['pred_scores'] > 0.5]))\n",
    "\n",
    "# confidence threshold of .5\n",
    "all_preds_df_50conf = all_preds_df[all_preds_df['pred_scores'] > 0.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66ae6538-6056-41ad-a78c-85fa8afa4054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>pred_boxes</th>\n",
       "      <th>pred_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9035</th>\n",
       "      <td>1238</td>\n",
       "      <td>[0.0, 0.0, 3408.0, 2800.0]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14192</th>\n",
       "      <td>1139</td>\n",
       "      <td>[0.0, 1610.4758, 1878.0297, 1923.6415]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8946</th>\n",
       "      <td>300</td>\n",
       "      <td>[1318.6195, 0.0, 1325.9933, 1760.0001]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8945</th>\n",
       "      <td>300</td>\n",
       "      <td>[1422.3435, 0.0, 1423.026, 1760.0001]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>625</td>\n",
       "      <td>[2714.6084, 492.90872, 2832.0, 1701.7156]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id                                 pred_boxes pred_scores\n",
       "9035       1238                 [0.0, 0.0, 3408.0, 2800.0]         1.0\n",
       "14192      1139     [0.0, 1610.4758, 1878.0297, 1923.6415]         1.0\n",
       "8946        300     [1318.6195, 0.0, 1325.9933, 1760.0001]         1.0\n",
       "8945        300      [1422.3435, 0.0, 1423.026, 1760.0001]         1.0\n",
       "1888        625  [2714.6084, 492.90872, 2832.0, 1701.7156]         1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d9bc204-8109-4a82-9763-8d89f698db72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1580\n",
      "14537\n"
     ]
    }
   ],
   "source": [
    "print(len(all_gt_boxes))\n",
    "print(len(all_preds_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "#\n",
    "#     LOAD FROM FILES\n",
    "#\n",
    "#####################################\n",
    "with open(\"all_preds.dat\", \"rb\") as file:\n",
    "    all_preds = pickle.load(file)\n",
    "with open(\"all_gt_boxes.dat\", \"rb\") as file:\n",
    "    all_gt_boxes = pickle.load(file)\n",
    "all_preds_df = pd.DataFrame(all_preds, columns=['image_id', 'pred_boxes', 'pred_scores'])    \n",
    "all_preds_df = all_preds_df.sort_values('pred_scores',  ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    image_id                                       pred_boxes  pred_scores\n17      4356  [0.18052556, 0.21400025, 0.47705406, 0.7356516]     0.690063\n1       3697  [0.07475066, 0.21586698, 0.38240293, 0.6904757]     0.681783\n18      4356   [0.62135273, 0.27515095, 0.8811942, 0.7460577]     0.667623\n10      1620  [0.60053945, 0.17989299, 0.87656426, 0.7830161]     0.652406\n0       5084  [0.66927755, 0.28014064, 0.8739166, 0.70936036]     0.630857",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>pred_boxes</th>\n      <th>pred_scores</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17</th>\n      <td>4356</td>\n      <td>[0.18052556, 0.21400025, 0.47705406, 0.7356516]</td>\n      <td>0.690063</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3697</td>\n      <td>[0.07475066, 0.21586698, 0.38240293, 0.6904757]</td>\n      <td>0.681783</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>4356</td>\n      <td>[0.62135273, 0.27515095, 0.8811942, 0.7460577]</td>\n      <td>0.667623</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1620</td>\n      <td>[0.60053945, 0.17989299, 0.87656426, 0.7830161]</td>\n      <td>0.652406</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>5084</td>\n      <td>[0.66927755, 0.28014064, 0.8739166, 0.70936036]</td>\n      <td>0.630857</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "all_preds_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "112f20f0-a4ad-41e2-b788-d9566a35706c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "14394\n"
     ]
    }
   ],
   "source": [
    "#all_gt_boxes = np.array(all_gt_boxes).transpose()[0]\n",
    "\n",
    "all_gt_boxes = all_gt_boxes_[:]\n",
    "\n",
    "MIN_IOU = 0.5\n",
    "\n",
    "#arrays for TP and FP         \n",
    "nd = len(all_preds_df)\n",
    "tp = [0] * nd # creates an array of zeros of size nd\n",
    "fp = [0] * nd\n",
    "all_tp = 0\n",
    "all_fp = 0\n",
    "\n",
    "#for each detected objct (detected_box)\n",
    "for i in range (len(all_preds_df)):\n",
    "    img_id = all_preds_df.iloc[i]['image_id']\n",
    "    pred_box = all_preds_df.iloc[i]['pred_boxes']\n",
    "\n",
    "    #count true positives:\n",
    "    # set temp_max_iou = -1\n",
    "    # set temp_gt_match = -1\n",
    "\n",
    "    iou_max = -1\n",
    "    gt_match = -1\n",
    "    gt_row = -1        \n",
    "    \n",
    "    #compare the detected_box with all gt_boxes for this image to find max IOU\n",
    "        #for box in gt boxes of image:\n",
    "                   \n",
    "                                           \n",
    "    '''\n",
    "    for idx, row in gt_boxes_i.iterrows():\n",
    "        bb_gt = row['gt_boxes']\n",
    "        #print(bb_gt)\n",
    "        bb_pred = pred['pred_boxes']\n",
    "        used = row['used']\n",
    "\n",
    "        #print(bb_pred)\n",
    "        #print()\n",
    "        #print(used)    \n",
    "    '''\n",
    "    #gt_boxes_i = all_gt_boxes[1][np.where(all_gt_boxes[0] == img_id)]\n",
    "    #used_flag_i = all_gt_boxes[2][np.where(all_gt_boxes[0] == img_id)]\n",
    "    \n",
    "    for j in range (len(all_gt_boxes[0])):\n",
    "        if all_gt_boxes[0][j] == img_id:\n",
    "            \n",
    "            bb_gt = all_gt_boxes[1][j]\n",
    "            used = all_gt_boxes[2][j]\n",
    "            \n",
    "            bb_pred = pred_box      \n",
    "            \n",
    "            if used == False:\n",
    "                # calculate IOU\n",
    "\n",
    "                intersect_box = [max(bb_pred[0],bb_gt[0]), max(bb_pred[1],bb_gt[1]), min(bb_pred[2],bb_gt[2]), min(bb_pred[3],bb_gt[3])]\n",
    "                #print(intersect_box)\n",
    "                intersect_w = intersect_box[2] - intersect_box[0] + 1\n",
    "                intersect_h = intersect_box[3] - intersect_box[1] + 1\n",
    "                if intersect_w > 0 and intersect_h >0:\n",
    "\n",
    "\n",
    "                    union_area = (bb_pred[2] - bb_pred[0] + 1) * (bb_pred[3] - bb_pred[1] + 1) + (bb_gt[2] - bb_gt[0] + 1) * (bb_gt[3] - bb_gt[1] + 1) - intersect_w * intersect_h\n",
    "\n",
    "                    iou_i = intersect_w * intersect_h / union_area\n",
    "                    #print(iou_i)\n",
    "\n",
    "                    # find max iou for this prediction\n",
    "                    if iou_i > iou_max:\n",
    "                        iou_max = iou_i\n",
    "                        gt_match = pred_box  \n",
    "                        iou_max_gt_box_idx = j\n",
    "                        #print(iou_max)\n",
    "\n",
    "\n",
    "    if iou_max >= MIN_IOU:\n",
    "        #print('yes!')\n",
    "        #set flag used to 'True'\n",
    "        all_gt_boxes[2][iou_max_gt_box_idx] = True\n",
    "        #used_flag_i = all_gt_boxes[2][np.where(all_gt_boxes[0] == img_id)]\n",
    "        #all_gt_boxes_df.at[gt_row.name, 'used'] = True\n",
    "        \n",
    "        tp[i] = 1\n",
    "        all_tp +=1\n",
    "    else:\n",
    "        fp[i] =1\n",
    "        all_fp +=1    \n",
    "    \n",
    "    # fp and tp arrays are sorted in same order as dataframe with predictions  \n",
    "\n",
    "    #calculate precision/recall\n",
    "    #cumsum = 0\n",
    "    \n",
    "\n",
    "#add cols with cumsum of tp and fp cols\n",
    "#add column rec: rec[idx] = float(tp[idx]) / gt_counter_per_class[class_name]\n",
    "#add column prec: prec[idx] = float(tp[idx]) / (fp[idx] + tp[idx])\n",
    "\n",
    "#ap, mrec, mprec = voc_ap(rec[:], prec[:])\n",
    "#siehe unten weiter\n",
    "\n",
    "print(all_tp)\n",
    "print(all_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df3b73e9-5e99-40ce-8c9f-4e8e1d4bf847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voc_ap(rec_, prec_):\n",
    "    \"\"\"\n",
    "    --- Official matlab code VOC2012---\n",
    "    mrec=[0 ; rec ; 1];\n",
    "    mpre=[0 ; prec ; 0];\n",
    "    for i=numel(mpre)-1:-1:1\n",
    "            mpre(i)=max(mpre(i),mpre(i+1));\n",
    "    end\n",
    "    i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
    "    ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n",
    "    \"\"\"\n",
    "    #rec.insert(0, 0.0) # insert 0.0 at begining of list\n",
    "    #rec.append(1.0) # insert 1.0 at end of list\n",
    "    mrec = np.concatenate([[0.0],rec_,[1.0]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #prec.insert(0, 0.0) # insert 0.0 at begining of list\n",
    "    #prec.append(0.0) # insert 0.0 at end of list\n",
    "    mpre = np.concatenate([[0.0],prec_ ,[0.0]])\n",
    "    #print(prec_)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "        \n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "   \n",
    "    \n",
    "    \"\"\"\n",
    "     This part makes the precision monotonically decreasing\n",
    "        (goes from the end to the beginning)\n",
    "        matlab: for i=numel(mpre)-1:-1:1\n",
    "                    mpre(i)=max(mpre(i),mpre(i+1));\n",
    "    \"\"\"\n",
    "    # matlab indexes start in 1 but python in 0, so I have to do:\n",
    "    #     range(start=(len(mpre) - 2), end=0, step=-1)\n",
    "    # also the python function range excludes the end, resulting in:\n",
    "    #     range(start=(len(mpre) - 2), end=-1, step=-1)\n",
    "    #for i in range(len(mpre)-2, -1, -1):\n",
    "        #mpre[i] = max(mpre[i], mpre[i+1])\n",
    "    #for i in range(mpre.size - 1, 0, -1):\n",
    "     #   mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "     This part creates a list of indexes where the recall changes\n",
    "        matlab: i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
    "    \n",
    "    i_list = []\n",
    "    for i in range(1, len(mrec)):\n",
    "        if mrec[i] != mrec[i-1]:\n",
    "            i_list.append(i) # if it was matlab would be i + 1\n",
    "    \"\"\"\n",
    "    '''\n",
    "     The Average Precision (AP) is the area under the curve\n",
    "        (numerical integration)\n",
    "        matlab: ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n",
    "    \n",
    "    ap = 0.0\n",
    "    for i in i_list:\n",
    "        ap += ((mrec[i]-mrec[i-1])*mpre[i])\n",
    "    '''\n",
    "\n",
    "    \n",
    "    return ap, mrec, mpre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3af8f496-282d-4560-a80c-1abbb59a56d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01% = opacity AP \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nrec = tp[:]\\nfor idx, val in enumerate(tp):\\n    rec[idx] = float(tp[idx]) / len(all_preds_df)\\n#print(rec)\\nprec = tp[:]\\nfor idx, val in enumerate(tp):\\n    prec[idx] = float(tp[idx]) / (fp[idx] + tp[idx])\\n#print(prec)\\n\\nap, mrec, mprec = voc_ap(rec[:], prec[:])\\nsum_AP += ap\\ntext = \"{0:.2f}%\".format(ap*100) + \" = \" + class_name + \" AP \" #class_name + \" AP = {0:.2f}%\".format(ap*100)\\n\\n#Write to output.txt\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tp = tp[:5862]\n",
    "#fp = fp[:5862]\n",
    "\n",
    "\n",
    "sum_AP = 0\n",
    "\n",
    "#rec = tp[:]\n",
    "#for i in range (len(tp)):\n",
    "    #rec[i] = float(tp[i]) / len(all_preds_df)\n",
    "    \n",
    "tp_cumsum = np.cumsum(tp, dtype=float)\n",
    "rec = tp_cumsum / len(tp)\n",
    "\n",
    "    \n",
    "#prec = tp[:]\n",
    "#for i in range (len(tp)):\n",
    "    #prec[i] = float(tp[i]) / (fp[i] + tp[i])\n",
    "fp_cumsum = np.cumsum(fp, dtype=float)\n",
    "prec = tp_cumsum /(fp_cumsum + tp_cumsum)\n",
    "\n",
    "\n",
    "    \n",
    "class_name = 'opacity'\n",
    "ap, mrec, mprec = voc_ap(rec[:], prec[:])\n",
    "\n",
    "#print(mprec)\n",
    "\n",
    "sum_AP += ap\n",
    "text = \"{0:.2f}%\".format(ap*100) + \" = \" + class_name + \" AP \" #class_name + \" AP = {0:.2f}%\".format(ap*100)\n",
    "\n",
    "\n",
    "print(text)\n",
    "    \n",
    "\"\"\"\n",
    "rec = tp[:]\n",
    "for idx, val in enumerate(tp):\n",
    "    rec[idx] = float(tp[idx]) / len(all_preds_df)\n",
    "#print(rec)\n",
    "prec = tp[:]\n",
    "for idx, val in enumerate(tp):\n",
    "    prec[idx] = float(tp[idx]) / (fp[idx] + tp[idx])\n",
    "#print(prec)\n",
    "\n",
    "ap, mrec, mprec = voc_ap(rec[:], prec[:])\n",
    "sum_AP += ap\n",
    "text = \"{0:.2f}%\".format(ap*100) + \" = \" + class_name + \" AP \" #class_name + \" AP = {0:.2f}%\".format(ap*100)\n",
    "\n",
    "#Write to output.txt\n",
    "\"\"\"\n",
    "\n",
    "#add cols with cumsum of tp and fp cols\n",
    "#add column rec: rec[idx] = float(tp[idx]) / gt_counter_per_class[class_name]\n",
    "#add column prec: prec[idx] = float(tp[idx]) / (fp[idx] + tp[idx])\n",
    "\n",
    "#ap, mrec, mprec = voc_ap(rec[:], prec[:])\n",
    "#siehe unten weiter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8522efa-e170-4c66-a7be-a2afe3cf2996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8981ebeeb0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mrec, mprec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f3aac0-b1ad-4caf-b365-9ac633cabfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9f13b6-fde2-470f-ac86-aa5572ac2701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c3e18-79ed-4b31-b0b2-cb816c1de4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247007f2-d0f5-40a8-aa0b-5c8268e22052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}